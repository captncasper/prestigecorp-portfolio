# Universal News Scraper

[![Live Demo](https://img.shields.io/badge/Live%20Demo-Visit%20Site-blue)](https://news-scraper-vercel-4zu8nqnxi-prestigecorp4s-projects.vercel.app)
[![GitHub](https://img.shields.io/badge/GitHub-Repository-black)](https://github.com/crusaders7/news-scraper-vercel)
[![Vercel](https://img.shields.io/badge/Vercel-Deployed-green)](https://vercel.com)

A sophisticated full-stack web scraping application built with Python and JavaScript that enables real-time article extraction, content processing, and multi-format downloading from multiple Australian news sources.

## üöÄ Live Demo

Experience the Universal News Scraper in action:
- **Live Application**: [news-scraper-vercel-4zu8nqnxi-prestigecorp4s-projects.vercel.app](https://news-scraper-vercel-4zu8nqnxi-prestigecorp4s-projects.vercel.app)
- **GitHub Repository**: [github.com/crusaders7/news-scraper-vercel](https://github.com/crusaders7/news-scraper-vercel)

## üìã Table of Contents

1. [Project Overview](#project-overview)
2. [Key Features](#key-features)
3. [Technical Architecture](#technical-architecture)
4. [Installation & Setup](#installation--setup)
5. [Vercel Deployment](#vercel-deployment)
6. [API Documentation](#api-documentation)
7. [Frontend Implementation](#frontend-implementation)
8. [Web Scraping Techniques](#web-scraping-techniques)
9. [Performance Optimization](#performance-optimization)
10. [Legal & Ethical Considerations](#legal--ethical-considerations)
11. [Portfolio Integration](#portfolio-integration)
12. [Troubleshooting](#troubleshooting)
13. [Future Enhancements](#future-enhancements)

## üéØ Project Overview

The Universal News Scraper is a cutting-edge web application that demonstrates advanced web scraping capabilities, serverless architecture, and modern web development practices. It serves as a comprehensive solution for journalists, researchers, and content creators who need to efficiently gather and process news content from multiple sources.

### Business Use Cases

- **Journalism & Media**: Gather breaking news and trending topics from multiple sources
- **Research & Analysis**: Collect articles for academic or market research
- **Content Creation**: Aggregate news for newsletters or blog posts
- **Competitive Intelligence**: Monitor news coverage across different publications
- **Social Media Management**: Find relevant articles for social media content

### Technical Highlights

- **Serverless Architecture**: Deployed on Vercel with edge computing for global performance
- **Multi-Source Scraping**: Intelligent article discovery across Australian news websites
- **Real-time Processing**: Live search and extraction with progress tracking
- **Clean Content Extraction**: Advanced parsing removes ads, navigation, and irrelevant content
- **Multiple Export Formats**: JSON for data analysis, ZIP for individual files
- **Responsive Design**: Optimized for all devices with modern UI/UX

## ‚ú® Key Features

### Core Functionality

- **üîç Smart Search**: Intelligent article discovery using multiple search strategies
- **üì∞ Multi-Source Support**: Scrapes from Illawarra Mercury, ABC News, and The Guardian Australia
- **üßπ Content Cleaning**: Removes ads, navigation, and subscription prompts
- **üìä Real-time Processing**: Live progress tracking and status updates
- **üì± Responsive Design**: Works seamlessly on desktop, tablet, and mobile
- **üíæ Export Options**: Download as JSON or ZIP with individual text files

### Advanced Features

- **‚ö° Edge Computing**: Vercel's edge functions for global performance
- **üõ°Ô∏è Rate Limiting**: Respects server resources with built-in delays
- **üîÑ Error Handling**: Robust error management and user feedback
- **üìà Progress Tracking**: Visual progress bars and status indicators
- **üé® Modern UI**: Clean, professional interface with loading states
- **üîó Cross-Domain Support**: Handles different website architectures

## üèóÔ∏è Technical Architecture

### System Architecture

```
Frontend (JavaScript/HTML/CSS)
    ‚Üì
Vercel Edge Functions (Python)
    ‚Üì
External News APIs & Websites
    ‚Üì
BeautifulSoup Content Processing
    ‚Üì
Structured Data Output
```

### Technology Stack

**Backend (Python)**
- **Framework**: Vercel Functions (Serverless)
- **Web Scraping**: BeautifulSoup4, Requests, lxml
- **Content Processing**: Regular expressions, datetime handling
- **Data Export**: JSON, ZIP file generation

**Frontend (JavaScript)**
- **Core**: Vanilla JavaScript (ES6+)
- **HTTP Client**: Fetch API
- **UI Framework**: TailwindCSS
- **DOM Manipulation**: Native JavaScript APIs

**Deployment & Infrastructure**
- **Hosting**: Vercel (Serverless platform)
- **Edge Computing**: Global CDN distribution
- **Functions**: Python runtime on Vercel
- **Domain**: Custom subdomain deployment

### API Endpoints

The application exposes three main serverless endpoints:

1. **`/api/search`** - Article discovery and URL collection
2. **`/api/scrape`** - Content extraction and processing
3. **`/api/download`** - File generation and export

## üõ†Ô∏è Installation & Setup

### Prerequisites

- **Python 3.8+** with pip
- **Node.js 16+** (for Vercel CLI)
- **Git** for version control
- **Vercel Account** for deployment

### Local Development Setup

1. **Clone the Repository**
   ```bash
   git clone https://github.com/crusaders7/news-scraper-vercel.git
   cd news-scraper-vercel
   ```

2. **Install Python Dependencies**
   ```bash
   pip install -r requirements.txt
   ```

3. **Install Vercel CLI**
   ```bash
   npm install -g vercel
   ```

4. **Run Local Development Server**
   ```bash
   vercel dev
   ```

5. **Access the Application**
   Open `http://localhost:3000` in your browser

### Project Structure

```
news-scraper-vercel/
‚îú‚îÄ‚îÄ api/
‚îÇ   ‚îú‚îÄ‚îÄ search.py      # Article search endpoint
‚îÇ   ‚îú‚îÄ‚îÄ scrape.py      # Content extraction endpoint
‚îÇ   ‚îî‚îÄ‚îÄ download.py    # File export endpoint
‚îú‚îÄ‚îÄ index.html         # Frontend interface
‚îú‚îÄ‚îÄ requirements.txt   # Python dependencies
‚îú‚îÄ‚îÄ vercel.json       # Vercel configuration
‚îî‚îÄ‚îÄ README.md         # This file
```

## üöÄ Vercel Deployment

### Automatic Deployment

1. **Connect to Vercel**
   ```bash
   vercel --prod
   ```

2. **Configure Environment** (if needed)
   ```bash
   vercel env add PYTHON_VERSION 3.9
   ```

3. **Deploy**
   ```bash
   vercel --prod
   ```

### Manual Deployment Steps

1. **Fork the Repository** on GitHub
2. **Connect to Vercel** Dashboard
3. **Import Project** from GitHub
4. **Configure Settings**:
   - Framework Preset: Other
   - Build Command: (leave empty)
   - Output Directory: (leave empty)
   - Install Command: pip install -r requirements.txt

### Vercel Configuration

```json
{
  "functions": {
    "api/search.py": {
      "runtime": "python3.9"
    },
    "api/scrape.py": {
      "runtime": "python3.9"
    },
    "api/download.py": {
      "runtime": "python3.9"
    }
  }
}
```

## üìö API Documentation

### Search Endpoint

**POST `/api/search`**

Searches for articles across multiple news sources based on query terms.

**Request Body:**
```json
{
  "query": "climate change",
  "max_results": 10,
  "sources": ["mercury", "abc", "guardian"]
}
```

**Response:**
```json
{
  "query": "climate change",
  "found": 8,
  "urls": [
    "https://www.illawarramercury.com.au/story/...",
    "https://www.abc.net.au/news/...",
    "https://www.theguardian.com/australia-news/..."
  ],
  "sources_searched": ["mercury", "abc", "guardian"]
}
```

**Parameters:**
- `query` (string, required): Search term or phrase
- `max_results` (integer, optional): Maximum articles to return (default: 10, max: 20)
- `sources` (array, optional): News sources to search (default: all sources)

### Scrape Endpoint

**POST `/api/scrape`**

Extracts content from provided article URLs.

**Request Body:**
```json
{
  "urls": ["https://example.com/article1", "https://example.com/article2"],
  "query": "climate change"
}
```

**Response:**
```json
{
  "query": "climate change",
  "scraped": 2,
  "articles": [
    {
      "id": 1,
      "url": "https://example.com/article1",
      "title": "Climate Change Impact on Australia",
      "date": "2024-01-15",
      "content": "Full article content here...",
      "scraped_at": "2024-01-15T10:30:00"
    }
  ]
}
```

### Download Endpoint

**POST `/api/download`**

Generates downloadable files from scraped articles.

**Request Body:**
```json
{
  "articles": [...],
  "format": "json"
}
```

**Response:** Binary file download (JSON or ZIP)

**Supported Formats:**
- `json`: Single JSON file with all articles
- `zip`: ZIP archive with individual text files + JSON

## üé® Frontend Implementation

### Modern JavaScript Architecture

The frontend uses vanilla JavaScript with modern ES6+ features for optimal performance and compatibility.

```javascript
// API Configuration
const API_BASE = window.location.hostname === 'localhost' 
    ? 'http://localhost:3000/api' 
    : '/api';

// Async/Await for API calls
async function searchArticles() {
    try {
        const response = await fetch(`${API_BASE}/search`, {
            method: 'POST',
            headers: { 'Content-Type': 'application/json' },
            body: JSON.stringify({ query, max_results: maxResults })
        });
        
        const data = await response.json();
        if (!response.ok) throw new Error(data.error);
        
        displayResults(data);
    } catch (error) {
        showError(error.message);
    }
}
```

### Responsive Design with TailwindCSS

```css
/* Custom utility classes */
.btn-primary {
    @apply bg-indigo-600 text-white font-bold py-3 px-6 rounded-lg 
           hover:bg-indigo-500 transition-all duration-300 ease-in-out 
           transform hover:scale-105 shadow-lg shadow-indigo-600/30;
}

.article-card {
    @apply bg-gray-800/30 border border-gray-700 rounded-lg p-6 
           hover:bg-gray-800/50 transition-all cursor-pointer;
}
```

### Interactive UI Components

- **Loading States**: CSS animations and spinners
- **Progress Bars**: Real-time scraping progress
- **Modal Viewers**: Article preview overlays
- **Error Handling**: User-friendly error messages
- **Responsive Grid**: Adaptive layout for all screen sizes

## üï∑Ô∏è Web Scraping Techniques

### Advanced Content Extraction

The scraping engine uses multiple strategies to extract clean article content:

```python
def extract_article_data(self, article_url, search_term=""):
    """Extract article data using multiple selectors"""
    content_selectors = [
        'div.story-content',      # Common news site structure
        'div.article-content',    # Alternative structure
        'article',                # Semantic HTML5
        'div[itemprop="articleBody"]'  # Schema.org markup
    ]
    
    for selector in content_selectors:
        content_elem = soup.select_one(selector)
        if content_elem:
            paragraphs = content_elem.find_all('p')
            if paragraphs:
                content = '\n\n'.join([p.text.strip() for p in paragraphs])
                break
```

### Content Cleaning Pipeline

```python
def clean_article_content(self, content):
    """Multi-stage content cleaning"""
    # 1. Unicode normalization
    content = content.replace('"', '"').replace('"', '"')
    
    # 2. Remove subscription prompts
    skip_phrases = [
        'your digital subscription',
        'access unlimited content',
        'login or signup to continue'
    ]
    
    # 3. Smart paragraph filtering
    paragraphs = [p.strip() for p in content.split('\n\n') if p.strip()]
    cleaned_paragraphs = []
    
    for para in paragraphs:
        if len(para) < 20:  # Skip short paragraphs
            continue
        if any(skip in para.lower() for skip in skip_phrases):
            continue
        cleaned_paragraphs.append(para)
    
    return '\n\n'.join(cleaned_paragraphs)
```

### Multi-Source Search Strategies

1. **Direct Website Search**: Query native search APIs
2. **Google Site Search**: Use Google to search specific domains
3. **RSS Feed Parsing**: Extract from RSS/Atom feeds
4. **Sitemap Crawling**: Discover articles through sitemaps

### Rate Limiting & Ethical Scraping

```python
# Respect server resources
time.sleep(1)  # 1-second delay between requests

# Limit concurrent requests
for i, url in enumerate(urls[:10]):  # Maximum 10 articles per request
    article_data = self.extract_article_data(url)
    if article_data:
        articles.append(article_data)
    time.sleep(1)  # Rate limiting
```

## ‚ö° Performance Optimization

### Serverless Architecture Benefits

- **Auto-scaling**: Handles traffic spikes automatically
- **Global Edge**: Reduced latency worldwide
- **Cost Efficiency**: Pay only for actual usage
- **Zero Maintenance**: No server management required

### Optimization Strategies

1. **Lazy Loading**: Load content only when needed
2. **Caching**: Browser caching for static assets
3. **Compression**: Gzip compression for API responses
4. **Parallel Processing**: Concurrent article processing
5. **Smart Timeouts**: Prevent hanging requests

### Performance Metrics

- **Search Response**: < 3 seconds for 10 articles
- **Scraping Speed**: ~1 article per second (with rate limiting)
- **File Generation**: < 5 seconds for ZIP/JSON export
- **Global Latency**: < 100ms via Vercel Edge Network

## ‚öñÔ∏è Legal & Ethical Considerations

### Responsible Scraping Practices

1. **Robots.txt Compliance**: Check and respect robots.txt files
2. **Rate Limiting**: Implement delays to avoid overwhelming servers
3. **Attribution**: Maintain original article URLs and sources
4. **Fair Use**: Use content for research, analysis, or news aggregation
5. **No Republishing**: Don't redistribute content without permission

### Legal Framework

- **Copyright**: Original content remains property of publishers
- **Fair Use**: Educational and research purposes are generally protected
- **Terms of Service**: Review individual website terms before scraping
- **Data Protection**: Handle personal data in compliance with privacy laws

### Ethical Guidelines

```python
# Example: Respectful scraping implementation
headers = {
    'User-Agent': 'Mozilla/5.0 (Educational Research Tool)',
    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8'
}

# Limit request frequency
time.sleep(1)  # Minimum 1-second delay between requests

# Respect server resources
timeout = 10  # Maximum wait time per request
```

## üéØ Portfolio Integration

### Integration with PrestigeCorp Portfolio

The Universal News Scraper seamlessly integrates with the main PrestigeCorp portfolio:

```javascript
// Navigation integration
<nav class="hidden md:flex space-x-8 items-center">
    <a href="/" class="hover:text-indigo-400 transition duration-300">Home</a>
    <a href="/#projects" class="hover:text-indigo-400 transition duration-300">Projects</a>
    <a href="https://github.com/captncasper" target="_blank">GitHub</a>
</nav>
```

### Consistent Design System

- **Color Palette**: Matches portfolio's indigo/gray theme
- **Typography**: Inter font family for consistency
- **Components**: Reusable UI components across projects
- **Responsive Grid**: Consistent layout patterns

### Cross-Project Navigation

The scraper includes navigation back to the main portfolio and other projects, creating a cohesive user experience across the entire PrestigeCorp ecosystem.

## üîß Troubleshooting

### Common Issues & Solutions

#### 1. API Timeout Errors
```
Error: Request timeout after 10 seconds
```
**Solution**: Reduce the number of articles requested or check internet connection.

#### 2. Empty Search Results
```
Error: No articles found for "query"
```
**Solutions**:
- Try different search terms
- Select different news sources
- Check if the news source is currently accessible

#### 3. Scraping Failures
```
Error: Failed to extract content from URL
```
**Solutions**:
- Website structure may have changed
- Site may be blocking automated requests
- Try again later or contact support

#### 4. Download Issues
```
Error: Download failed - no articles to download
```
**Solution**: Ensure articles are successfully scraped before attempting download.

### Debug Mode

Enable debug mode for detailed error information:

```javascript
// Add to browser console for debugging
localStorage.setItem('debug', 'true');
```

### Performance Troubleshooting

1. **Slow Search**: Try reducing max_results parameter
2. **Scraping Timeout**: Some websites may be slow to respond
3. **Large Downloads**: ZIP files with many articles may take time to generate

## üöÄ Future Enhancements

### Planned Features

#### Phase 1: Enhanced Functionality
- [ ] **More News Sources**: Expand to include more Australian and international news sites
- [ ] **Advanced Filtering**: Date range, keyword density, article length filters
- [ ] **Content Summarization**: AI-powered article summarization
- [ ] **Duplicate Detection**: Identify and merge similar articles from different sources

#### Phase 2: Advanced Analytics
- [ ] **Sentiment Analysis**: Analyze article sentiment and tone
- [ ] **Topic Modeling**: Automatic categorization and topic extraction
- [ ] **Trend Analysis**: Identify trending topics and themes
- [ ] **Source Reliability**: Score sources based on factual accuracy

#### Phase 3: User Experience
- [ ] **User Accounts**: Save searches and bookmark articles
- [ ] **API Access**: RESTful API for developers
- [ ] **Scheduled Scraping**: Automated daily/weekly scraping
- [ ] **Email Alerts**: Notifications for new articles on specific topics

#### Phase 4: Enterprise Features
- [ ] **Team Collaboration**: Share articles and insights with team members
- [ ] **Custom Sources**: Add private or specialized news sources
- [ ] **Advanced Export**: PDF, Word, and other format exports
- [ ] **Analytics Dashboard**: Comprehensive reporting and analytics

### Technical Improvements

1. **Caching Layer**: Redis/CDN caching for faster responses
2. **Database Integration**: PostgreSQL for article storage and history
3. **Machine Learning**: Improve content extraction accuracy
4. **Real-time Updates**: WebSocket connections for live updates
5. **Mobile App**: React Native mobile application

### Scalability Roadmap

- **Microservices Architecture**: Split into smaller, focused services
- **Container Deployment**: Docker containers for easier deployment
- **Load Balancing**: Handle high-traffic scenarios
- **Global Distribution**: Multiple regions for better performance

## üìä Project Statistics

### Development Metrics
- **Lines of Code**: ~1,200 (Python: 700, JavaScript: 350, CSS: 150)
- **API Endpoints**: 3 serverless functions
- **News Sources**: 3 Australian news websites
- **Export Formats**: 2 (JSON, ZIP)
- **Development Time**: 40+ hours

### Performance Benchmarks
- **Search Speed**: 2-5 seconds for 10 articles
- **Scraping Rate**: 1 article per second (with rate limiting)
- **Success Rate**: 85-95% depending on source availability
- **Uptime**: 99.9% (Vercel infrastructure)

---

## ü§ù Contributing

We welcome contributions to the Universal News Scraper! Here's how you can help:

### Ways to Contribute

1. **Report Bugs**: Use GitHub issues for bug reports
2. **Suggest Features**: Share ideas for new functionality
3. **Improve Documentation**: Help make the docs better
4. **Add News Sources**: Contribute scrapers for new websites
5. **Fix Issues**: Submit pull requests for bug fixes

### Development Setup

1. Fork the repository
2. Create a feature branch: `git checkout -b feature/new-feature`
3. Make your changes and test thoroughly
4. Submit a pull request with a clear description

## üìÑ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## üìû Contact & Support

- **GitHub Issues**: [Report bugs or request features](https://github.com/crusaders7/news-scraper-vercel/issues)
- **Email**: [contact@prestigecorp.com](mailto:contact@prestigecorp.com)
- **Portfolio**: [PrestigeCorp Portfolio](https://prestigecorp.com)

---

**Built with ‚ù§Ô∏è by [PrestigeCorp](https://prestigecorp.com)**

*This project demonstrates advanced web scraping techniques, serverless architecture, and modern web development practices. It serves as both a practical tool for news aggregation and a showcase of technical expertise in Python, JavaScript, and cloud deployment.*